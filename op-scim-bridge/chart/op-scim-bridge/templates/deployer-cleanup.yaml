apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: "{{ .Values.name }}-cleanup" # deletes stale deployers to improve upgradeability of the SCIM bridge
  namespace: {{ .Values.namespace }}
  labels: &CronLabels
    app.kubernetes.io/name: {{ .Values.name }}
    app.kubernetes.io/component: cleanup
spec:
  schedule: "*/1 * * * *" # once a minute
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      backoffLimit: 10
      template:
        metadata:
          labels: *CronLabels
        spec:
          serviceAccountName: {{ .Values.scim.serviceAccount }}
          containers:
          - name: kubectl-runner
            image: bitnami/kubectl
            volumeMounts:
            - name: config-volume
              mountPath: /cleanup
            command: ["/bin/bash"]
            args: ["/cleanup/delete-deployer.sh"]
          restartPolicy: Never
          volumes:
          - name: config-volume
            configMap:
              name: "{{ .Values.name }}-cleanup-script"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: "{{ .Values.name }}-cleanup-script"
  namespace: {{ .Values.namespace }}
  labels:
    app.kubernetes.io/name: "{{ .Values.name }}"
    app.kubernetes.io/component: cleanup-script
data:
    delete-deployer.sh: |-
      set -e
      deployer_job_name=$(kubectl get jobs -n {{ .Values.namespace }} | awk '$1 ~ /{{ .Values.name }}-deployer/ && $2 ~ /1.*1/ { print $1 }' 2>/dev/null) # determine if the deployer is running
      completed=$(kubectl -n {{ .Values.namespace }} describe job $deployer_job_name | grep 'Completed At' 2>/dev/null)
      echo "checking for deployer"
      if [[ "$completed" ]]; then
        echo "deployer detected, deleting"
        kubectl delete jobs -n {{ .Values.namespace }} $deployer_job_name 2>/dev/null; # delete the job with the correct name
      else
        echo "deployer not detected, skipping"
      fi
      exit 0
