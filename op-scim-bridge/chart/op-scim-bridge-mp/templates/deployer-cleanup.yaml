--- # service account for cleanup jon
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "{{ .Values.name }}-cleanup-sa"
  namespace: {{ .Values.namespace }}
--- # permissions for cleanup job
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "{{ .Values.name }}-cleanup-role"
  namespace: {{ .Values.namespace }}
rules:
  - apiGroups:
      - "" # "" indicates the core API group
      - batch
    resources:
      - jobs
    verbs: ["get", "list", "delete"]
--- # add permissions to service account
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "{{ .Values.name }}-cleanup-rolebinding"
  namespace: {{ .Values.namespace }} 
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "{{ .Values.name }}-cleanup-role"
subjects:
- namespace: {{ .Values.namespace }} 
  kind: ServiceAccount
  name: "{{ .Values.name }}-cleanup-sa"
--- # clean up job definition
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: "{{ .Values.name }}-cleanup" # deletes stale deployers to improve upgradeability of the SCIM bridge
  namespace: {{ .Values.namespace }}
  labels: &CronLabels
    app.kubernetes.io/name: {{ .Values.name }}
    app.kubernetes.io/component: cleanup
spec:
  schedule: "*/10 * * * *" # every 10 minutes
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      parallelism: 1 # how many pods will be instantiated at once
      completions: 1 # how many containers of the job are instantiated one after the other (sequentially) inside the pod
      backoffLimit: 1 # maximum pod restarts in case of failure
      activeDeadlineSeconds: 300 # limit the time for which a Job can continue to run; 5 minutes
      template:
        metadata:
          labels: *CronLabels
        spec:
          serviceAccountName: "{{ .Values.name }}-cleanup-sa"
          containers:
          - name: kubectl-runner
            image: bitnami/kubectl
            volumeMounts:
            - name: config-volume
              mountPath: /cleanup
            command: ["/bin/bash"]
            args: ["/cleanup/delete-deployer.sh"]
          restartPolicy: Never
          terminationGracePeriodSeconds: 30
          volumes:
          - name: config-volume
            configMap:
              name: "{{ .Values.name }}-cleanup-script"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: "{{ .Values.name }}-cleanup-script"
  namespace: {{ .Values.namespace }}
  labels:
    app.kubernetes.io/name: "{{ .Values.name }}"
    app.kubernetes.io/component: cleanup-script
data:
  delete-deployer.sh: |-
    set -e
    
    deployer_job=$(kubectl get jobs -n {{ .Values.namespace }} | awk -v name={{ .Values.name }}-deployer '$1 ~ name && $2 ~ /1.*1/ { print $1 }' 2>/dev/null)
    echo "checking for deployer job"
    if [[ "$deployer_job" ]]; then
      echo "deployer job found: $deployer_job"
    else
      echo "no deployer job found, skipping"
      exit 0
    fi

    completed_at=$(kubectl -n {{ .Values.namespace }} describe job $deployer_job | grep 'Completed At' 2>/dev/null)
    echo "checking if deployer job completed"
    if [[ "$completed_at" ]]; then
      echo "deployer job completed, deleting"
      kubectl delete jobs -n {{ .Values.namespace }} $deployer_job
    else
      echo "deployer job not completed, skipping"
    fi

    exit 0
